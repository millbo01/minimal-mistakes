---
layout: single
title: "Choice"
---

## **Choice (Meta)**

**Tag:** Meta, Decision, Selection, Optimization, Agency, Behavior generation

**Definition:**

Selection among perceived options under current tier weights and constraints.

**Full Explanation:**

Choice is the observable output of the optimization process. Actor perceives option set, current tier weights determine values, constraints limit feasibility, choice is the argmax of that function. It's not "free will"—it's mechanical output of [option set + tier weights + constraints + model].

Three components determine choice:

1. **Option set:** What actions feel available (see [[Option set]])
2. **Tier weights:** What outcomes matter most right now (see [[Dominant tier]])
3. **Constraints:** What's actually feasible (see [[Constraint]])

"Choice" feels like agency from inside, but it's deterministic from outside. Given complete information about those three inputs, choice is predictable. The appearance of freedom comes from the actor not having introspective access to their own optimization function—they experience the output without seeing the mechanism.

This doesn't make choice meaningless—it just means "why did you choose X?" has a mechanical answer involving tier weights, perceived options, and constraints, not a story about character or free will.

**Why "choice" language persists:** Because optimization is fast and unconscious. By the time you're aware of "choosing," the algorithm already ran. Consciousness narrates the output as "I decided" when really "the system selected based on inputs."

**Example 1:** Person "chooses" junk food over healthy meal. Feels like free choice. Mechanical reality: T3 (stress) + T4 (time scarcity) + option set (doesn't include easy healthy options) + model (believes healthy food takes too long) = junk food scores highest. Not a character failure, not weakness—just optimization output.

**Example 2:** Employee "chooses" not to report safety issue. Feels like personal decision. Mechanical reality: T3 (fear of retaliation) + T2 (reputation protection) + perceived option set (doesn't include anonymous reporting or protected whistleblowing) + model (believes reporting = job loss) = silence scores highest. Same optimization, different context would produce different choice.

**Related constructs:**

→ [[Option set]], [[Dominant tier]], [[Constraint]], [[Agency]], [[Free will]], [[Local rationality]], [[Behavior]]

**Intervention principle:**

Change choice by changing inputs—either expand option set, shift dominant tier, remove constraints, or update model. Trying to change "the choice" directly by appealing to willpower fails because you're fighting the optimization function instead of changing its inputs.
